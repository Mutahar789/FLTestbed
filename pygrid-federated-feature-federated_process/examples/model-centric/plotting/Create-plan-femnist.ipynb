{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Setting up Sandbox...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch as th\n",
    "from torch import nn\n",
    "\n",
    "import syft as sy\n",
    "from syft.serde import protobuf\n",
    "from syft_proto.execution.v1.plan_pb2 import Plan as PlanPB\n",
    "from syft_proto.execution.v1.state_pb2 import State as StatePB\n",
    "from syft.grid.clients.model_centric_fl_client import ModelCentricFLClient\n",
    "from syft.execution.state import State\n",
    "from syft.execution.placeholder import PlaceHolder\n",
    "from syft.execution.translation import TranslationTarget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import visualization_utils\n",
    "\n",
    "from baseline_constants import (\n",
    "    ACCURACY_KEY,\n",
    "    BYTES_READ_KEY,\n",
    "    BYTES_WRITTEN_KEY,\n",
    "    CLIENT_ID_KEY,\n",
    "    LOCAL_COMPUTATIONS_KEY,\n",
    "    NUM_ROUND_KEY,\n",
    "    NUM_SAMPLES_KEY)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from websocket import create_connection\n",
    "import websockets\n",
    "import json\n",
    "import requests\n",
    "from functools import reduce\n",
    "import random\n",
    "\n",
    "sy.make_hook(globals())\n",
    "hook.local_worker.framework = None # force protobuf serialization for tensors\n",
    "seed = 1549774894\n",
    "th.random.manual_seed(seed)\n",
    "th.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_model_params(module, params_list, start_param_idx=0):\n",
    "    \"\"\" Set params list into model recursively\n",
    "    \"\"\"\n",
    "    param_idx = start_param_idx\n",
    "\n",
    "    for name, param in module._parameters.items():\n",
    "        module._parameters[name] = params_list[param_idx]\n",
    "        param_idx += 1\n",
    "\n",
    "    for name, child in module._modules.items():\n",
    "        if child is not None:\n",
    "            param_idx = set_model_params(child, params_list, param_idx)\n",
    "\n",
    "    return param_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_vs_round_number(stat_metrics, weighted=True):\n",
    "    if weighted:\n",
    "        accuracies = stat_metrics.groupby(NUM_ROUND_KEY).apply(_weighted_mean, ACCURACY_KEY, NUM_SAMPLES_KEY)\n",
    "        accuracies = accuracies.reset_index(name=ACCURACY_KEY)\n",
    "\n",
    "    else:\n",
    "        accuracies = stat_metrics.groupby(NUM_ROUND_KEY, as_index=False).mean()\n",
    "        stds = stat_metrics.groupby(NUM_ROUND_KEY, as_index=False).std()\n",
    "\n",
    "    percentile_10 = stat_metrics.groupby(NUM_ROUND_KEY, as_index=False).apply(lambda x: x.quantile(0.10)) #.quantile(10),\n",
    "    percentile_90 = stat_metrics.groupby(NUM_ROUND_KEY, as_index=False).apply(lambda x: x.quantile(0.90)) #.quantile(90),\n",
    "\n",
    "    return accuracies, percentile_10, percentile_90\n",
    "\n",
    "def _weighted_mean(df, metric_name, weight_name):\n",
    "    d = df[metric_name]\n",
    "    w = df[weight_name]\n",
    "\n",
    "    try:\n",
    "        return (w * d).sum() / w.sum()\n",
    "    except ZeroDivisionError:\n",
    "        return np.nan\n",
    "\n",
    "def plot_accuracy_vs_round_number(realx_axis_values, \n",
    "                                  realy_axis_values,\n",
    "                                  leafx_axis_values, \n",
    "                                  leafy_axis_values, \n",
    "                                  atype, \n",
    "                                  weighted=False, figsize=(8, 6), title_fontsize=16, **kwargs):\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    title_weighted = 'Weighted' if weighted else 'Unweighted'\n",
    "    plt.title(atype + ' Accuracy vs Round Number (%s)' % title_weighted, fontsize=title_fontsize)\n",
    "#     plt.suptitle(\"Seed: 1549774894, Clients: 2, Epochs:10, Batch:20\", y=1, fontsize=15)\n",
    "    \n",
    "    plt.plot(realx_axis_values, realy_axis_values, label='Testbed')\n",
    "    plt.plot(leafx_axis_values, leafy_axis_values, label='LEAF')\n",
    "    \n",
    "    max_y1 = np.amax(realy_axis_values)\n",
    "    max_y2 = np.amax(leafy_axis_values)\n",
    "    \n",
    "    ylimit = min(max(max_y1, max_y2) + 0.1, 1)\n",
    "    \n",
    "    plt.legend(loc='best', fontsize=25)\n",
    "\n",
    "    plt.ylabel('Accuracy', fontsize=25)\n",
    "    plt.xlabel('Round Number', fontsize=25)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "#     plt.ylim(0,0.8)\n",
    "    \n",
    "#     print(fig.axes)\n",
    "    \n",
    "    plt.savefig('fig.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "/home/mutahar789/ENVs/PyGrid/lib/python3.7/site-packages/torch/__init__.py\n",
      "/home/mutahar789/ENVs/PyGrid/lib/python3.7/site-packages/syft/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# c = np.load('transposed_np_weights_1549774894.npy',allow_pickle=True)\n",
    "print(th.__version__)\n",
    "print(th.__file__)\n",
    "print(sy.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FemnistNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FemnistNet, self).__init__()\n",
    "        # 1.6 Million\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2) ##output shape (batch, 32, 28, 28)self.pool1 = nn.MaxPool2d(2, stride=2) ## output shape (batch, 32, 14, 14)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2) ## output shape (batch, 64, 7, 7)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2) ##output shape (batch, 64, 14, 14)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2) ## output shape (batch, 64, 7, 7)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1568, 1024) ##input = 32 x 4 x 4 for without padding, 32 x 7 x 7=padding\n",
    "        self.fc2 = nn.Linear(1024 ,62) ##input of [BatchSize, 2048]. output of [BatchSize, 62]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = th.nn.functional.relu(x)\n",
    "\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        print(\"pool1 shape\", x.shape)\n",
    "\n",
    "        x=self.conv2(x)\n",
    "        x = th.nn.functional.relu(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        print(\"pool2 shape\", x.shape)\n",
    "        \n",
    "        x = x.flatten(start_dim=1)\n",
    "        print(\"flatten shape\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        l1_activations = th.nn.functional.relu(x)\n",
    "        \n",
    "        x = self.fc2(l1_activations)\n",
    "\n",
    "        return x, l1_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool1 shape torch.Size([20, 16, 14, 14])\n",
      "pool2 shape torch.Size([20, 32, 7, 7])\n",
      "flatten shape torch.Size([20, 1568])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             416\n",
      "         MaxPool2d-2           [-1, 16, 14, 14]               0\n",
      "            Conv2d-3           [-1, 32, 14, 14]          12,832\n",
      "         MaxPool2d-4             [-1, 32, 7, 7]               0\n",
      "            Linear-5                 [-1, 1024]       1,606,656\n",
      "            Linear-6                   [-1, 62]          63,550\n",
      "================================================================\n",
      "Total params: 1,683,454\n",
      "Trainable params: 1,683,454\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 0.19\n",
      "Params size (MB): 6.42\n",
      "Estimated Total Size (MB): 6.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "large_model = FemnistNet()\n",
    "# from torchvision import models\n",
    "from torchsummary import summary\n",
    "summary(large_model, (10, 28, 28))\n",
    "\n",
    "\n",
    "# from fvcore.nn import FlopCountAnalysis\n",
    "# flops = FlopCountAnalysis(large_model, (10, 28, 28))\n",
    "# flops.total()\n",
    "\n",
    "# from thop import profile\n",
    "\n",
    "# macs, params = profile(large_model, inputs=(1, 28, 28))\n",
    "# print(macs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FemnistNetSmall(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FemnistNetSmall, self).__init__()\n",
    "        # 1.6 Million\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2) ##output shape (batch, 32, 28, 28)self.pool1 = nn.MaxPool2d(2, stride=2) ## output shape (batch, 32, 14, 14)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2) ## output shape (batch, 64, 7, 7)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2) ##output shape (batch, 64, 14, 14)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2) ## output shape (batch, 64, 7, 7)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1568, 1024) ##input = 32 x 4 x 4 for without padding, 32 x 7 x 7=padding\n",
    "        self.fc2 = nn.Linear(1024 ,62) ##input of [BatchSize, 2048]. output of [BatchSize, 62]\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = th.nn.functional.relu(x)\n",
    "\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        print(\"pool1 shape\", x.shape)\n",
    "\n",
    "        x=self.conv2(x)\n",
    "        x = th.nn.functional.relu(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        print(\"pool2 shape\", x.shape)\n",
    "        \n",
    "        x = x.flatten(start_dim=1)\n",
    "        print(\"flatten shape\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        l1_activations = th.nn.functional.relu(x)\n",
    "        \n",
    "        x = self.fc2(l1_activations)\n",
    "\n",
    "        return x, l1_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool1 shape torch.Size([2, 16, 14, 14])\n",
      "pool2 shape torch.Size([2, 32, 7, 7])\n",
      "flatten shape torch.Size([2, 1568])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             416\n",
      "         MaxPool2d-2           [-1, 16, 14, 14]               0\n",
      "            Conv2d-3           [-1, 32, 14, 14]          12,832\n",
      "         MaxPool2d-4             [-1, 32, 7, 7]               0\n",
      "            Linear-5                 [-1, 1024]       1,606,656\n",
      "            Linear-6                   [-1, 62]          63,550\n",
      "================================================================\n",
      "Total params: 1,683,454\n",
      "Trainable params: 1,683,454\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.19\n",
      "Params size (MB): 6.42\n",
      "Estimated Total Size (MB): 6.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "small_model = FemnistNetSmall()\n",
    "# from torchvision import models\n",
    "from torchsummary import summary\n",
    "summary(small_model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_entropy_with_logits(log_logits, targets, batch_size):\n",
    "#     eps = PlaceHolder().on(th.tensor(1e-7), wrap = False)\n",
    "#     values = (targets * th.log(log_logits + eps))\n",
    "#     reduced_values =  values.sum()\n",
    "    \n",
    "#     print(\"values shape\", values.shape)\n",
    "#     print(\"reduced values\", reduced_values)\n",
    "#     print(\"reduced values shape\", reduced_values.shape)\n",
    "    \n",
    "#     return - reduced_values/ batch_size\n",
    "\n",
    "\n",
    "# def cross_entropy_with_logits(log_logits, targets, batch_size):\n",
    "#     eps = PlaceHolder().on(th.tensor(1e-7), wrap = False)\n",
    "#     return -(targets * th.log(log_logits + eps)).sum() / batch_size\n",
    "\n",
    "\n",
    "def softmax_cross_entropy_with_logits(logits, targets, batch_size):\n",
    "    \"\"\" Calculates softmax entropy\n",
    "        Args:\n",
    "            * logits: (NxC) outputs of dense layer\n",
    "            * targets: (NxC) one-hot encoded labels\n",
    "            * batch_size: value of N, temporarily required because Plan cannot trace .shape\n",
    "    \"\"\"\n",
    "    # numstable logsoftmax\n",
    "    norm_logits = logits - logits.max(dim = 1, keepdim = True)[0]\n",
    "    log_probs = norm_logits - norm_logits.exp().sum(dim=1, keepdim=True).log()\n",
    "    \n",
    "    # NLL, reduction = mean\n",
    "    return -(targets * log_probs).sum() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def naive_sgd(param, **kwargs):\n",
    "    return param - kwargs['lr'] * param.grad\n",
    "#     return param - kwargs['lr'] * (param.grad * th.tensor(0.) + th.tensor(1.))\n",
    "#     return param - 0 * (param.grad + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.random.manual_seed(seed)\n",
    "th.manual_seed(seed)\n",
    "model = FemnistNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = [model_param.data for model_param in model.parameters()]\n",
    "\n",
    "# weights_converted = np.load('converted_th_model.npy',allow_pickle=True)\n",
    "# for item_converted, param in zip(weights_converted, model_params):\n",
    "#     transposed_tensor = th.from_numpy(item_converted)\n",
    "#     param.data.copy_(transposed_tensor)\n",
    "    \n",
    "# print(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_over_examples(activations, total_examples):\n",
    "    reduced_sum = th.sum(activations, dim=0)#reduce(th.add, activations)\n",
    "    return th.div(reduced_sum, total_examples)\n",
    "\n",
    "\n",
    "@sy.func2plan()\n",
    "def training_plan(X, y, batch_size, lr, model_params):\n",
    "    model.train()\n",
    "    \n",
    "    # inject params into model\n",
    "    set_model_params(model, model_params)\n",
    "    \n",
    "    \n",
    "    logits, activations = model.forward(X)\n",
    "    loss = softmax_cross_entropy_with_logits(logits, y, batch_size)\n",
    "    \n",
    "#     l2_penalty = 0.001 * sum([(p**2).sum() for p in model_params])\n",
    "#     loss_with_penalty = loss + l2_penalty\n",
    "#     print(type(loss))\n",
    "    loss.backward()\n",
    "    \n",
    "    updated_params = [\n",
    "        naive_sgd(param, lr=lr)\n",
    "        for param in model_params\n",
    "    ]\n",
    "        \n",
    "#     gradients = [th.max(param.grad) for param in model_params]\n",
    "#     np.set_printoptions(threshold=np.inf)\n",
    "# #     np_weights = np.array(gradients)\n",
    "#     np.save('gradients_1', gradients)\n",
    "\n",
    "    # accuracy\n",
    "    pred = th.argmax(logits, dim=1)\n",
    "    target = th.argmax(y, dim=1)\n",
    "    acc = pred.eq(target).sum()/ batch_size\n",
    "    \n",
    "#     print(\"Acc:\", acc)\n",
    "    \n",
    "    avg_act_over_examples = get_average_over_examples(activations, list(X.shape)[0])\n",
    "    \n",
    "    \n",
    "    return (\n",
    "        loss,\n",
    "        acc,\n",
    "        logits,\n",
    "        avg_act_over_examples,\n",
    "        *updated_params,        \n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool1 shape torch.Size([10, 16, 14, 14])\n",
      "pool2 shape torch.Size([10, 32, 7, 7])\n",
      "flatten shape torch.Size([10, 1568])\n"
     ]
    }
   ],
   "source": [
    "num = 10\n",
    "\n",
    "dataX = th.tensor(np.zeros((num,784)), dtype=th.float) ##784 sized 1D array. would be reshaped to 1, 28, 28\n",
    "dataY = th.tensor(np.zeros(num, dtype=np.int64)) ##62-length sized one hot vectors\n",
    "\n",
    "# th.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# dataX = th.rand((num,784), dtype=th.float) ##784 sized 1D array. would be reshaped to 1, 28, 28\n",
    "# dataY = th.tensor(np.zeros(num, dtype=np.int64)) ##62-length sized one hot vectors\n",
    "\n",
    "X = th.tensor((dataX), dtype=th.float) ##784 sized 1D array. would be reshaped to 1, 28, 28\n",
    "y = nn.functional.one_hot(th.tensor(dataY), 62) ##62-length sized one hot vectors\n",
    "\n",
    "lr = th.tensor([0.0003]) ##0.0003 learning rate\n",
    "batch_size = th.tensor([float(num)]) ##20 is our batch size\n",
    "\n",
    "loss, acc, logits, avg_act_over_examples, *updated_params = training_plan.build(X[:num], y[:num], batch_size, lr, model_params, trace_autograd=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.random.manual_seed(seed)\n",
    "th.manual_seed(seed)\n",
    "model_small = FemnistNetSmall() ##model\n",
    "model_params_small = [param.data for param in model_small.parameters()]  # raw tensors instead of nn.Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool1 shape torch.Size([10, 16, 14, 14])\n",
      "pool2 shape torch.Size([10, 32, 7, 7])\n",
      "flatten shape torch.Size([10, 1568])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@sy.func2plan()\n",
    "def training_plan_small(X, y, batch_size, lr, model_params_small):\n",
    "    model_small.train()\n",
    "    \n",
    "    # inject params into model\n",
    "    set_model_params(model_small, model_params_small)\n",
    "    \n",
    "    \n",
    "    logits, activations = model_small.forward(X)\n",
    "    loss = softmax_cross_entropy_with_logits(logits, y, batch_size)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    updated_params = [\n",
    "        naive_sgd(param, lr=lr)\n",
    "        for param in model_params_small\n",
    "    ]\n",
    "\n",
    "    # accuracy\n",
    "    pred = th.argmax(logits, dim=1)\n",
    "    target = th.argmax(y, dim=1)\n",
    "    acc = pred.eq(target).sum()/ batch_size\n",
    "    \n",
    "#     print(\"Acc:\", acc)\n",
    "    \n",
    "    avg_act_over_examples = get_average_over_examples(activations, list(X.shape)[0])\n",
    "    \n",
    "    \n",
    "    return (\n",
    "        loss,\n",
    "        acc,\n",
    "        logits,\n",
    "        avg_act_over_examples,\n",
    "        *updated_params,        \n",
    "    )\n",
    "\n",
    "loss, acc, logits, avg_act_over_examples, *updated_params = training_plan_small.build(X[:num], y[:num], batch_size, lr, model_params_small, trace_autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.func2plan()\n",
    "def convert_to_one_hot_plan(input_data):\n",
    "    one_hot_labels = nn.functional.one_hot(input_data, 62)\n",
    "    return one_hot_labels\n",
    "    \n",
    "input_label_data = th.tensor([1,1,1,1,1,1,1,1,1,1])\n",
    "_ = convert_to_one_hot_plan.build(input_label_data)\n",
    "#print(convert_to_one_hot_plan(input_label_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Averaging Plan\n",
    "\n",
    "Averaging Plan is executed by PyGrid at the end of the cycle,\n",
    "to average _diffs_ submitted by workers and update the model\n",
    "and create new checkpoint for the next cycle.\n",
    "\n",
    "_Diff_ is the difference between client-trained\n",
    "model params and original model params,\n",
    "so it has same number of tensors and tensor's shapes\n",
    "as the model parameters.\n",
    "\n",
    "We define Plan that processes one diff at a time.\n",
    "Such Plans require `iterative_plan` flag set to `True`\n",
    "in `server_config` when hosting FL model to PyGrid.\n",
    "\n",
    "Plan below will calculate simple mean of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.func2plan()\n",
    "def avg_plan(avg, item, num):\n",
    "    new_avg = []\n",
    "    for i, param in enumerate(avg):\n",
    "        new_avg.append((avg[i] * num + item[i]) / (num + 1))\n",
    "    return new_avg\n",
    "\n",
    "# Build the Plan\n",
    "_ = avg_plan.build(model_params, model_params, th.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test averaging plan\n",
    "# Pretend there're diffs, all params of which are ones * dummy_coeffs\n",
    "# dummy_coeffs = [1., 5.5, 7, 55]\n",
    "dummy_coeffs = [1.]\n",
    "dummy_diffs = [[th.ones_like(param) * i for param in model_params] for i in dummy_coeffs]\n",
    "mean_coeff = th.tensor(dummy_coeffs).mean().item()\n",
    "\n",
    "# Remove original function to make sure we execute traced Plan\n",
    "avg_plan.forward = None\n",
    "\n",
    "# Calculate avg value using our plan\n",
    "avg = dummy_diffs[0]\n",
    "for i, diff in enumerate(dummy_diffs[1:]):\n",
    "    avg = avg_plan(list(avg), diff, th.tensor([i + 1]))\n",
    "    \n",
    "# Avg should be ones*mean_coeff for each param\n",
    "for i, param in enumerate(model_params):\n",
    "    expected = th.ones_like(param) * mean_coeff\n",
    "    assert avg[i].eq(expected).all(), f\"param #{i}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "gridAddress = \"127.0.0.1:5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.func2plan()\n",
    "def sum_activations(old_activations, new_activations):\n",
    "    summed_tensor = th.add(old_activations, new_activations)\n",
    "    return th.div(summed_tensor, 2) # Sum of tensors should be divided by 2 because we are taking average \n",
    "    \n",
    "old = th.tensor([ [0.1,  0.2]\n",
    "                  ])\n",
    "new = th.tensor([ [0.7,  0.8]\n",
    "                  ])\n",
    "\n",
    "_ = sum_activations.build(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.func2plan()\n",
    "def average_activations(activations, num_rounds):\n",
    "    return th.div(activations, num_rounds)\n",
    "\n",
    "\n",
    "num_rounds = th.tensor([10])\n",
    "_ = average_activations.build(old, num_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool1 shape torch.Size([10, 16, 14, 14])\n",
      "pool2 shape torch.Size([10, 32, 7, 7])\n",
      "flatten shape torch.Size([10, 1568])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "@sy.func2plan()\n",
    "def evaluate_model_plan(X, y, batch_size, model_params):\n",
    "    model.eval()\n",
    "    # Load model params into the model\n",
    "    set_model_params(model, model_params)\n",
    "    \n",
    "    # Test\n",
    "    logits, activations = model(X)\n",
    "    \n",
    "    preds = th.argmax(logits, dim=1)\n",
    "    target = th.argmax(y, dim=1)\n",
    "    \n",
    "    acc = preds.eq(target).sum().float() / batch_size\n",
    "    loss = softmax_cross_entropy_with_logits(logits, y, batch_size)\n",
    "   \n",
    "    return acc, loss\n",
    "\n",
    "test_x = th.tensor((dataX), dtype=th.float)\n",
    "test_y = nn.functional.one_hot(th.tensor(dataY), 62)\n",
    "\n",
    "_ = evaluate_model_plan.build(test_x[:num], test_y[:num], th.tensor(float(num)), model_params, trace_autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stats_file():\n",
    "    req = requests.get(f\"http://{gridAddress}/model-centric/training-metrics\")\n",
    "\n",
    "    body = json.loads(req.content)\n",
    "\n",
    "    stats = body.get('data').get('stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"mnist\" \n",
    "version = \"1.0\"\n",
    "is_pruning_enabled = 1\n",
    "if is_pruning_enabled == True:\n",
    "    optimizer = \"hasaas\"\n",
    "else:\n",
    "    optimizer = \"fedavg\"\n",
    "    \n",
    "bootstrap_rounds = 10\n",
    "is_fedavg = 1\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "learning_rate = 0.0003\n",
    "client_drop_rate = 3/4\n",
    "model_drop_rate = 0\n",
    "rounds = 10\n",
    "\n",
    "is_force_pruning_enabled = 0\n",
    "goal_count = 4\n",
    "min_workers = goal_count\n",
    "min_diffs = goal_count\n",
    "\n",
    "timeout = 150000 # 15 minutes for pruned model with 6.6M parameters\n",
    "\n",
    "client_config = {\n",
    "    \"name\": name,\n",
    "    \"version\": version,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"lr\": learning_rate,\n",
    "    \"seed\": seed,\n",
    "    \"bootstrap_rounds\": bootstrap_rounds,\n",
    "    \"max_updates\": epochs,  # custom syft.js option that limits number of training loops per worker\n",
    "    \"optimizer\": optimizer,\n",
    "    \"cycle_length\": timeout,\n",
    "    \n",
    "}\n",
    "\n",
    "server_config = {\n",
    "    \"min_workers\": min_workers, # minimum number of clients for a pool\n",
    "    \"max_workers\": 5, # Maximum number of clients who can register\n",
    "    \"pool_selection\": \"random\",\n",
    "    \"do_not_reuse_workers_until_cycle\": 6,\n",
    "    \"cycle_length\": timeout,  \n",
    "    \"num_cycles\": rounds,  # max number of cycles\n",
    "    \"min_diffs\": min_diffs,  # number of diffs to collect before avg\n",
    "    \"max_diffs\": 1,  # number of diffs to collect before avg\n",
    "    \"worker_participation_mode\": 0, # set 1 = participate anytime in the cycle 0 = participate after cycle completion\n",
    "    \"goal_count\": goal_count, # Number of clients to pick from pool\n",
    "    \"minimum_upload_speed\":0,\n",
    "    \"minimum_download_speed\": 0,\n",
    "    \"bootstrap_rounds\": bootstrap_rounds, # After how many rounds the pruning should occur\n",
    "    \"drop_rate\": client_drop_rate, # how many clients should be slow\n",
    "    \"prune_percentage\": model_drop_rate, # percentage of neurons to be pruned\n",
    "    \"iterative_plan\": False,  # tells PyGrid that avg plan is executed per diff\n",
    "    \"seed\": seed,\n",
    "    \"is_pruning_enabled\": is_pruning_enabled,\n",
    "    \"is_force_pruning\": is_force_pruning_enabled,\n",
    "    \"optimizer\": optimizer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Authentication (optional)\n",
    "Let's additionally protect the model with simple authentication for workers.\n",
    "\n",
    "PyGrid supports authentication via JWT token (HMAC, RSA) or opaque token\n",
    "via remote API.\n",
    "\n",
    "We'll try JWT/RSA. Suppose we generate RSA keys:\n",
    "```\n",
    "openssl genrsa -out private.pem\n",
    "openssl rsa -in private.pem -pubout -out public.pem\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "private_key = \"\"\"\n",
    "-----BEGIN RSA PRIVATE KEY-----\n",
    "MIIEowIBAAKCAQEAzQMcI09qonB9OZT20X3Z/oigSmybR2xfBQ1YJ1oSjQ3YgV+G\n",
    "FUuhEsGDgqt0rok9BreT4toHqniFixddncTHg7EJzU79KZelk2m9I2sEsKUqEsEF\n",
    "lMpkk9qkPHhJB5AQoClOijee7UNOF4yu3HYvGFphwwh4TNJXxkCg69/RsvPBIPi2\n",
    "9vXFQzFE7cbN6jSxiCtVrpt/w06jJUsEYgNVQhUFABDyWN4h/67M1eArGA540vyd\n",
    "kYdSIEQdknKHjPW62n4dvqDWxtnK0HyChsB+LzmjEnjTJqUzr7kM9Rzq3BY01DNi\n",
    "TVcB2G8t/jICL+TegMGU08ANMKiDfSMGtpz3ZQIDAQABAoIBAD+xbKeHv+BxxGYE\n",
    "Yt5ZFEYhGnOk5GU/RRIjwDSRplvOZmpjTBwHoCZcmsgZDqo/FwekNzzuch1DTnIV\n",
    "M0+V2EqQ0TPJC5xFcfqnikybrhxXZAfpkhtU+gR5lDb5Q+8mkhPAYZdNioG6PGPS\n",
    "oGz8BsuxINhgJEfxvbVpVNWTdun6hLOAMZaH3DHgi0uyTBg8ofARoZP5RIbHwW+D\n",
    "p+5vd9x/x7tByu76nd2UbMp3yqomlB5jQktqyilexCIknEnfb3i/9jqFv8qVE5P6\n",
    "e3jdYoJY+FoomWhqEvtfPpmUFTY5lx4EERCb1qhWG3a7sVBqTwO6jJJBsxy3RLIS\n",
    "Ic0qZcECgYEA6GsBP11a2T4InZ7cixd5qwSeznOFCzfDVvVNI8KUw+n4DOPndpao\n",
    "TUskWOpoV8MyiEGdQHgmTOgGaCXN7bC0ERembK0J64FI3TdKKg0v5nKa7xHb7Qcv\n",
    "t9ccrDZVn4y/Yk5PCqjNWTR3/wDR88XouzIGaWkGlili5IJqdLEvPvUCgYEA4dA+\n",
    "5MNEQmNFezyWs//FS6G3lTRWgjlWg2E6BXXvkEag6G5SBD31v3q9JIjs+sYdOmwj\n",
    "kfkQrxEtbs173xgYWzcDG1FI796LTlJ/YzuoKZml8vEF3T8C4Bkbl6qj9DZljb2j\n",
    "ehjTv5jA256sSUEqOa/mtNFUbFlBjgOZh3TCsLECgYAc701tdRLdXuK1tNRiIJ8O\n",
    "Enou26Thm6SfC9T5sbzRkyxFdo4XbnQvgz5YL36kBnIhEoIgR5UFGBHMH4C+qbQR\n",
    "OK+IchZ9ElBe8gYyrAedmgD96GxH2xAuxAIW0oDgZyZgd71RZ2iBRY322kRJJAdw\n",
    "Xq77qo6eXTKpni7grjpijQKBgDHWRAs5DVeZkTwhoyEW0fRfPKUxZ+ZVwUI9sxCB\n",
    "dt3guKKTtoY5JoOcEyJ9FdBC6TB7rV4KGiSJJf3OXAhgyP9YpNbimbZW52fhzTuZ\n",
    "bwO/ZWC40RKDVZ8f63cNsiGz37XopKvNzu36SJYv7tY8C5WvvLsrd/ZxvIYbRUcf\n",
    "/dgBAoGBAMdR5DXBcOWk3+KyEHXw2qwWcGXyzxtca5SRNLPR2uXvrBYXbhFB/PVj\n",
    "h3rGBsiZbnIvSnSIE+8fFe6MshTl2Qxzw+F2WV3OhhZLLtBnN5qqeSe9PdHLHm49\n",
    "XDce6NV2D1mQLBe8648OI5CScQENuRGxF2/h9igeR4oRRsM1gzJN\n",
    "-----END RSA PRIVATE KEY-----\n",
    "\"\"\".strip()\n",
    "\n",
    "public_key = \"\"\"\n",
    "-----BEGIN PUBLIC KEY-----\n",
    "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAzQMcI09qonB9OZT20X3Z\n",
    "/oigSmybR2xfBQ1YJ1oSjQ3YgV+GFUuhEsGDgqt0rok9BreT4toHqniFixddncTH\n",
    "g7EJzU79KZelk2m9I2sEsKUqEsEFlMpkk9qkPHhJB5AQoClOijee7UNOF4yu3HYv\n",
    "GFphwwh4TNJXxkCg69/RsvPBIPi29vXFQzFE7cbN6jSxiCtVrpt/w06jJUsEYgNV\n",
    "QhUFABDyWN4h/67M1eArGA540vydkYdSIEQdknKHjPW62n4dvqDWxtnK0HyChsB+\n",
    "LzmjEnjTJqUzr7kM9Rzq3BY01DNiTVcB2G8t/jICL+TegMGU08ANMKiDfSMGtpz3\n",
    "ZQIDAQAB\n",
    "-----END PUBLIC KEY-----\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we set __public key__ into model authentication config,\n",
    "then PyGrid will validate that submitted JWT auth token is signed with private key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "server_config[\"authentication\"] = {\n",
    "    \"type\": \"jwt\",\n",
    "    \"pub_key\": public_key,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we're ready to host our federated Training Plan!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = ModelCentricFLClient(id=\"test\", address=gridAddress, secure=False)\n",
    "grid.connect() # These name/version you use in worker\n",
    "model_params_state = State(\n",
    "    state_placeholders=[\n",
    "        PlaceHolder().instantiate(param)\n",
    "        for param in model_params\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Un-comment for pruning configuration\n",
    "response = grid.host_federated_training(\n",
    "    model=model_params_state,\n",
    "    client_plans={'training_plan': training_plan, 'training_plan_small': training_plan_small, \"evaluate_model_plan\": evaluate_model_plan, \"convert_to_one_hot_plan\": convert_to_one_hot_plan,\"sum_activations\": sum_activations, \"average_activations\":average_activations},\n",
    "    client_protocols={},\n",
    "    server_averaging_plan=avg_plan,\n",
    "    client_config=client_config,\n",
    "    server_config=server_config\n",
    ")\n",
    "\n",
    "# un-comment for no pruning\n",
    "# response = grid.host_federated_training(\n",
    "#     model=model_params_state,\n",
    "#     client_plans={'training_plan': training_plan, \"evaluate_model_plan\": evaluate_model_plan, \"convert_to_one_hot_plan\": convert_to_one_hot_plan,\"sum_activations\": sum_activations, \"average_activations\":average_activations},\n",
    "#     client_protocols={},\n",
    "#     server_averaging_plan=avg_plan,\n",
    "#     client_config=client_config,\n",
    "#     server_config=server_config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1549774894\n"
     ]
    }
   ],
   "source": [
    "build_stats_file()\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "PyGrid",
   "language": "python",
   "name": "pygrid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
