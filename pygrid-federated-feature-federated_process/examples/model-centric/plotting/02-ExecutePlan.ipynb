{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning - Model Centric MNIST Example: Train FL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the \"[01-Create Plan](../model-centric/02-ExecutePlan.ipynb)\" notebook we created the model, training plan, and averaging plan, and then hosted all of them in PyGrid.\n",
    "\n",
    "Such hosted FL model can be now trained using client libraries, SwiftSyft, KotlinSyft, syft.js.\n",
    "\n",
    "In this notebook, we'll use FL Client included in the PySyft to do the training.\n",
    "\n",
    "### Credits:\n",
    "- Original authors: \n",
    "\n",
    " - Vova Manannikov - Github: [@vvmnnnkv](https://github.com/vvmnnnkv)\n",
    "\n",
    "\n",
    "- Reviewers: \n",
    " - Patrick Cason - Github: [@cereallcerny](https://github.com/cereallarceny)\n",
    "\n",
    "\n",
    "- New Content tested and enriched by: \n",
    " - Juan M. Aunon - Twitter: [@jm_aunon](https://twitter.com/jm_aunon) - Github: [@jmaunon](https://github.com/jmaunon)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Sandbox...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch as th\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import urllib3\n",
    "import time\n",
    "\n",
    "import syft as sy\n",
    "from syft.federated.fl_client import FLClient\n",
    "from syft.federated.fl_job import FLJob\n",
    "from syft.grid.clients.model_centric_fl_client import ModelCentricFLClient\n",
    "\n",
    "urllib3.disable_warnings()\n",
    "sy.make_hook(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_key = \"\"\"\n",
    "-----BEGIN RSA PRIVATE KEY-----\n",
    "MIIEowIBAAKCAQEAzQMcI09qonB9OZT20X3Z/oigSmybR2xfBQ1YJ1oSjQ3YgV+G\n",
    "FUuhEsGDgqt0rok9BreT4toHqniFixddncTHg7EJzU79KZelk2m9I2sEsKUqEsEF\n",
    "lMpkk9qkPHhJB5AQoClOijee7UNOF4yu3HYvGFphwwh4TNJXxkCg69/RsvPBIPi2\n",
    "9vXFQzFE7cbN6jSxiCtVrpt/w06jJUsEYgNVQhUFABDyWN4h/67M1eArGA540vyd\n",
    "kYdSIEQdknKHjPW62n4dvqDWxtnK0HyChsB+LzmjEnjTJqUzr7kM9Rzq3BY01DNi\n",
    "TVcB2G8t/jICL+TegMGU08ANMKiDfSMGtpz3ZQIDAQABAoIBAD+xbKeHv+BxxGYE\n",
    "Yt5ZFEYhGnOk5GU/RRIjwDSRplvOZmpjTBwHoCZcmsgZDqo/FwekNzzuch1DTnIV\n",
    "M0+V2EqQ0TPJC5xFcfqnikybrhxXZAfpkhtU+gR5lDb5Q+8mkhPAYZdNioG6PGPS\n",
    "oGz8BsuxINhgJEfxvbVpVNWTdun6hLOAMZaH3DHgi0uyTBg8ofARoZP5RIbHwW+D\n",
    "p+5vd9x/x7tByu76nd2UbMp3yqomlB5jQktqyilexCIknEnfb3i/9jqFv8qVE5P6\n",
    "e3jdYoJY+FoomWhqEvtfPpmUFTY5lx4EERCb1qhWG3a7sVBqTwO6jJJBsxy3RLIS\n",
    "Ic0qZcECgYEA6GsBP11a2T4InZ7cixd5qwSeznOFCzfDVvVNI8KUw+n4DOPndpao\n",
    "TUskWOpoV8MyiEGdQHgmTOgGaCXN7bC0ERembK0J64FI3TdKKg0v5nKa7xHb7Qcv\n",
    "t9ccrDZVn4y/Yk5PCqjNWTR3/wDR88XouzIGaWkGlili5IJqdLEvPvUCgYEA4dA+\n",
    "5MNEQmNFezyWs//FS6G3lTRWgjlWg2E6BXXvkEag6G5SBD31v3q9JIjs+sYdOmwj\n",
    "kfkQrxEtbs173xgYWzcDG1FI796LTlJ/YzuoKZml8vEF3T8C4Bkbl6qj9DZljb2j\n",
    "ehjTv5jA256sSUEqOa/mtNFUbFlBjgOZh3TCsLECgYAc701tdRLdXuK1tNRiIJ8O\n",
    "Enou26Thm6SfC9T5sbzRkyxFdo4XbnQvgz5YL36kBnIhEoIgR5UFGBHMH4C+qbQR\n",
    "OK+IchZ9ElBe8gYyrAedmgD96GxH2xAuxAIW0oDgZyZgd71RZ2iBRY322kRJJAdw\n",
    "Xq77qo6eXTKpni7grjpijQKBgDHWRAs5DVeZkTwhoyEW0fRfPKUxZ+ZVwUI9sxCB\n",
    "dt3guKKTtoY5JoOcEyJ9FdBC6TB7rV4KGiSJJf3OXAhgyP9YpNbimbZW52fhzTuZ\n",
    "bwO/ZWC40RKDVZ8f63cNsiGz37XopKvNzu36SJYv7tY8C5WvvLsrd/ZxvIYbRUcf\n",
    "/dgBAoGBAMdR5DXBcOWk3+KyEHXw2qwWcGXyzxtca5SRNLPR2uXvrBYXbhFB/PVj\n",
    "h3rGBsiZbnIvSnSIE+8fFe6MshTl2Qxzw+F2WV3OhhZLLtBnN5qqeSe9PdHLHm49\n",
    "XDce6NV2D1mQLBe8648OI5CScQENuRGxF2/h9igeR4oRRsM1gzJN\n",
    "-----END RSA PRIVATE KEY-----\n",
    "\"\"\".strip()\n",
    "\n",
    "public_key = \"\"\"\n",
    "-----BEGIN PUBLIC KEY-----\n",
    "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAzQMcI09qonB9OZT20X3Z\n",
    "/oigSmybR2xfBQ1YJ1oSjQ3YgV+GFUuhEsGDgqt0rok9BreT4toHqniFixddncTH\n",
    "g7EJzU79KZelk2m9I2sEsKUqEsEFlMpkk9qkPHhJB5AQoClOijee7UNOF4yu3HYv\n",
    "GFphwwh4TNJXxkCg69/RsvPBIPi29vXFQzFE7cbN6jSxiCtVrpt/w06jJUsEYgNV\n",
    "QhUFABDyWN4h/67M1eArGA540vydkYdSIEQdknKHjPW62n4dvqDWxtnK0HyChsB+\n",
    "LzmjEnjTJqUzr7kM9Rzq3BY01DNiTVcB2G8t/jICL+TegMGU08ANMKiDfSMGtpz3\n",
    "ZQIDAQAB\n",
    "-----END PUBLIC KEY-----\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating authentication token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.e30.Cn_0cSjCw1QKtcYDx_mYN_q9jO2KkpcUoiVbILmKVB4LUCQvZ7YeuyQ51r9h3562KQoSas_ehbjpz2dw1Dk24hQEoN6ObGxfJDOlemF5flvLO_sqAHJDGGE24JRE4lIAXRK6aGyy4f4kmlICL6wG8sGSpSrkZlrFLOVRJckTptgaiOTIm5Udfmi45NljPBQKVpqXFSmmb3dRy_e8g3l5eBVFLgrBhKPQ1VbNfRK712KlQWs7jJ31fGpW2NxMloO1qcd6rux48quivzQBCvyK8PV5Sqrfw_OMOoNLcSvzePDcZXa2nPHSu3qQIikUdZIeCnkJX-w0t8uEFG3DfH1fVA\n"
     ]
    }
   ],
   "source": [
    "import jwt\n",
    "auth_token = jwt.encode({}, private_key, algorithm='RS256').decode('ascii')\n",
    "\n",
    "print(auth_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define `on_accepted`, `on_rejected`, `on_error` handlers.\n",
    "\n",
    "The main training loop is located inside `on_accepted` routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "def batch_data(data, batch_size, seed):\n",
    "    \n",
    "    '''\n",
    "    data is a dict := {'x': [numpy array], 'y': [numpy array]} (on one client)\n",
    "    returns x, y, which are both numpy array of length: batch_size\n",
    "    '''\n",
    "    data_x = data['x']\n",
    "    data_y = data['y']\n",
    "\n",
    "    # loop through mini-batches\n",
    "    for i in range(0, len(data_x), batch_size):\n",
    "        end = len(data_x) if i + batch_size > len(data_x) else i + batch_size\n",
    "        batched_x = data_x[i:end]\n",
    "        batched_y = data_y[i:end]\n",
    "        yield (batched_x, batched_y)\n",
    "\n",
    "\n",
    "def read_dir(data_dir):\n",
    "    clients = []\n",
    "    groups = []\n",
    "    data = defaultdict(lambda : None)\n",
    "\n",
    "    files = os.listdir(data_dir)\n",
    "    files = [f for f in files if f.endswith('.json')]\n",
    "    for f in files:\n",
    "        file_path = os.path.join(data_dir,f)\n",
    "        with open(file_path, 'r') as inf:\n",
    "            cdata = json.load(inf)\n",
    "        clients.extend(cdata['users'])\n",
    "        if 'hierarchies' in cdata:\n",
    "            groups.extend(cdata['hierarchies'])\n",
    "        data.update(cdata['user_data'])\n",
    "\n",
    "    clients = list(sorted(data.keys()))\n",
    "    return clients, groups, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(train_data_dir, test_data_dir):\n",
    "    '''parses data in given train and test data directories\n",
    "\n",
    "    assumes:\n",
    "    - the data in the input directories are .json files with \n",
    "        keys 'users' and 'user_data'\n",
    "    - the set of train set users is the same as the set of test set users\n",
    "    \n",
    "    Return:\n",
    "        clients: list of client ids\n",
    "        groups: list of group ids; empty list if none found\n",
    "        train_data: dictionary of train data\n",
    "        test_data: dictionary of test data\n",
    "    '''\n",
    "    train_clients, train_groups, train_data = read_dir(train_data_dir)\n",
    "    test_clients, test_groups, test_data = read_dir(test_data_dir)\n",
    "    assert train_clients == test_clients\n",
    "    assert train_groups == test_groups\n",
    "\n",
    "    return train_clients, train_groups, train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['f0016_39', 'custom1'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# train_data_dir = os.path.join('.', 'data', 'femnist', 'train')\n",
    "# test_data_dir = os.path.join('.', 'data', 'femnist', 'test')\n",
    "train_data_dir = os.path.join('.', 'data_custom', 'train')\n",
    "test_data_dir = os.path.join('.', 'data_custom', 'test')\n",
    "users, groups, train_data, test_data = read_data(train_data_dir, test_data_dir)\n",
    "# print(len(train_data['f4071_32']['x']))\n",
    "print(train_data.keys())\n",
    "# users = ['f4074_12', 'f4071_32']\n",
    "# users = ['f4071_32']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 20\n",
    "\n",
    "# for batched_x, batched_y in batch_data(train_data['f4071_32'], batch_size, seed=1234):\n",
    "#     print(len(batched_x))\n",
    "\n",
    "# #     input_data = self.process_x(batched_x)\n",
    "# #     target_data = self.process_y(batched_y)\n",
    "\n",
    "test_X = th.tensor(np.array(test_data['custom1']['x']), dtype=th.float)\n",
    "test_y = th.nn.functional.one_hot(th.tensor(test_data['custom1']['y'], dtype=th.int64), 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cycles_log = []\n",
    "training_plan = None\n",
    "testing_plan = None\n",
    "model_params = None\n",
    "status = {\n",
    "    \"ended\": False\n",
    "}\n",
    "\n",
    "# Called when client is accepted into FL cycle\n",
    "def on_accepted(job: FLJob):\n",
    "    global training_plan\n",
    "    global testing_plan\n",
    "    global model_params\n",
    "    \n",
    "    print(f\"Accepted into cycle {len(cycles_log) + 1}!\")\n",
    "\n",
    "    cycle_params = job.client_config\n",
    "    batch_size = cycle_params[\"batch_size\"]\n",
    "    lr = cycle_params[\"lr\"]\n",
    "    max_updates = cycle_params[\"max_updates\"]\n",
    "#     print(job)\n",
    "#     print(batch_size)\n",
    "#     mnist_dataset = th.utils.data.DataLoader(\n",
    "#         datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "#         batch_size=batch_size,\n",
    "#         drop_last=True,\n",
    "#         shuffle=True,\n",
    "#     )\n",
    "\n",
    "    training_plan = job.plans[\"training_plan\"]\n",
    "    testing_plan = job.plans[\"evaluate_model_plan\"]\n",
    "    model_params = job.model.tensors()\n",
    "    status['ended'] = True\n",
    "    return\n",
    "#     print(model_params[0][:5])\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "#     for batch_idx, (X, y) in enumerate(mnist_dataset):\n",
    "#         X = X.view(batch_size, -1)\n",
    "    \n",
    "    print(' '.join([str(model_param.sum()) for model_param in model_params]))\n",
    "    \n",
    "    test_acc,test_loss = testing_plan.torchscript(test_X, test_y, th.tensor(len(test_y)), model_params)\n",
    "    print(f'test_acc {test_acc} test_loss {test_loss}')\n",
    "    for i in range(max_updates):\n",
    "        for batched_x, batched_y in batch_data(train_data['custom1'], batch_size, seed=1234):\n",
    "            X = th.tensor(np.array(batched_x), dtype=th.float)\n",
    "            y = th.tensor(np.array(batched_y), dtype=th.int64)\n",
    "\n",
    "            y_oh = th.nn.functional.one_hot(y, 62)\n",
    "            loss, acc, logits, target, *model_params = training_plan.torchscript(\n",
    "                X, y_oh, th.tensor(batch_size), th.tensor(lr), model_params\n",
    "            )\n",
    "#             [print(model_param.sum()) for model_param in model_params]\n",
    "            \n",
    "#             print(len(updated_params))\n",
    "#             print(len(model_params))\n",
    "            \n",
    "#             model_params = updated_params\n",
    "#   \n",
    "    print(' '.join([str(model_param.sum()) for model_param in model_params]))\n",
    "    \n",
    "    test_acc,test_loss = testing_plan.torchscript(test_X, test_y, th.tensor(len(test_y)), model_params)\n",
    "    print(f'test_acc_AFTER {test_acc} test_loss_AFTER {test_loss}')\n",
    "\n",
    "    \n",
    "    job.report(model_params)\n",
    "    cycles_log.append((test_loss, test_acc))\n",
    "    \n",
    "# Called when the client is rejected from cycle\n",
    "def on_rejected(job: FLJob, timeout):\n",
    "    if timeout is None:\n",
    "        print(f\"Rejected from cycle without timeout (this means FL training is done)\")\n",
    "    else:\n",
    "        print(f\"Rejected from cycle with timeout: {timeout}\")\n",
    "    status[\"ended\"] = True\n",
    "\n",
    "# Called when error occured\n",
    "def on_error(job: FLJob, error: Exception):\n",
    "    print(f\"Error: {error}\")\n",
    "    status[\"ended\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sjks'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(['sj', 'ks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We use same PyGrid Node where the model was hosted, the model name/version of hosted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# PyGrid Node address\n",
    "# gridAddress = \"ws://alice:5000\"\n",
    "gridAddress = \"ws://localhost:5000\"\n",
    "\n",
    "# Hosted model name/version\n",
    "model_name = \"mnist\"\n",
    "model_version = \"1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define routine that creates FL client and starts the FL process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.grid_worker.get_connection_speed(client.worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_job(self, model_name, model_version) -> FLJob:\n",
    "        if self.worker_id is None:\n",
    "            auth_response = self.grid_worker.authenticate(\n",
    "                self.auth_token, model_name, model_version\n",
    "            )\n",
    "            self.worker_id = auth_response[\"data\"][\"worker_id\"]\n",
    "\n",
    "        job = FLJob(\n",
    "            fl_client=self,\n",
    "            grid_worker=self.grid_worker,\n",
    "            model_name=model_name,\n",
    "            model_version=model_version,\n",
    "        )\n",
    "        return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_and_run_cycle():\n",
    "    client = FLClient(url=gridAddress, auth_token=auth_token, verbose=True)\n",
    "    authResponse = client.grid_worker.authenticate(client.auth_token,model_name,model_version)\n",
    "#     printprint(acc.item())(authResponse)\n",
    "    client.worker_id = authResponse[\"data\"][\"worker_id\"]\n",
    "    job = client.new_job(model_name, model_version)\n",
    "\n",
    "    # Set event handlers\n",
    "    job.add_listener(job.EVENT_ACCEPTED, on_accepted)\n",
    "    job.add_listener(job.EVENT_REJECTED, on_rejected)\n",
    "    job.add_listener(job.EVENT_ERROR, on_error)\n",
    "\n",
    "    # Shoot!\n",
    "    job.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we're ready to start FL training.\n",
    "\n",
    "We're going to run multiple \"workers\" until the FL model is fully done and see the progress.\n",
    "\n",
    "As we create & authenticate new client each time,\n",
    "this emulates multiple different workers requesting a cycle and working on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted into cycle 1!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while not status[\"ended\"]:\n",
    "    create_client_and_run_cycle()\n",
    "    print('\\n\\n')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_copy = model_params.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc 0.009999999776482582 test_loss 10.483942985534668\n",
      "test_acc 0.4350000023841858 test_loss 2.0518555641174316\n",
      "test_acc 0.5174999833106995 test_loss 1.7015289068222046\n",
      "test_acc 0.5550000071525574 test_loss 1.5512460470199585\n",
      "test_acc 0.5724999904632568 test_loss 1.479295015335083\n",
      "test_acc 0.5849999785423279 test_loss 1.4390881061553955\n",
      "test_acc 0.5849999785423279 test_loss 1.4247289896011353\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "batch_size = 10\n",
    "lr = 0.0003\n",
    "def run_epoch(model_params_copy):\n",
    "    test_acc,test_loss = testing_plan.torchscript(test_X, test_y, th.tensor(len(test_y)), model_params_copy)\n",
    "    print(f'test_acc {test_acc} test_loss {test_loss}')\n",
    "    total_batches = 1\n",
    "    for i in range(math.ceil(total_batches)):\n",
    "        for batched_x, batched_y in batch_data(train_data['custom1'], batch_size, seed=1234):\n",
    "            X = th.tensor(np.array(batched_x), dtype=th.float)\n",
    "            y = th.tensor(np.array(batched_y), dtype=th.int64)\n",
    "\n",
    "            y_oh = th.nn.functional.one_hot(y, 62)\n",
    "            loss, acc, logits, target, *model_params_copy = training_plan.torchscript(\n",
    "                X, y_oh, th.tensor(batch_size), th.tensor(lr), model_params_copy\n",
    "            )\n",
    "    return model_params_copy, test_acc, test_loss\n",
    "\n",
    "test_accs = []\n",
    "test_losses = []\n",
    "for i in range(50):\n",
    "    model_params_copy, test_acc, test_loss = run_epoch(model_params_copy)\n",
    "    test_accs.append(test_acc)\n",
    "    test_losses.append(test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's plot loss and accuracy statistics recorded from each worker.\n",
    "Each such worker's statistics is drawn with different color.\n",
    "\n",
    "It's visible that loss/accuracy improvement occurs after each `max_diffs` reports to PyGrid,\n",
    "because PyGrid updates the model and creates new checkpoint after\n",
    "receiving `max_diffs` updates from FL clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (3.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (1.18.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(cycles_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import visualization_utils\n",
    "\n",
    "from baseline_constants import (\n",
    "    ACCURACY_KEY,\n",
    "    BYTES_READ_KEY,\n",
    "    BYTES_WRITTEN_KEY,\n",
    "    CLIENT_ID_KEY,\n",
    "    LOCAL_COMPUTATIONS_KEY,\n",
    "    NUM_ROUND_KEY,\n",
    "    NUM_SAMPLES_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/metrics_stat_testbed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-651c4da8946c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0msys_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'metrics_sys.csv'\u001b[0m \u001b[0;31m# change to None if desired\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mfstat_metrics_testbed\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvisualization_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_file_testbed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mfstat_metrics\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvisualization_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/pygrid-federated/examples/model-centric/visualization_utils.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(stat_metrics_file)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_metrics_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'metrics_stat.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m\"\"\"Loads the data from the given stat_metric and sys_metric files.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mstat_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_metrics_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstat_metrics_file\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstat_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    648\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/metrics_stat_testbed.csv'"
     ]
    }
   ],
   "source": [
    "def get_accuracy_vs_round_number(stat_metrics, weighted=False):\n",
    "    if weighted:\n",
    "        accuracies = stat_metrics.groupby(NUM_ROUND_KEY).apply(_weighted_mean, ACCURACY_KEY, NUM_SAMPLES_KEY)\n",
    "        accuracies = accuracies.reset_index(name=ACCURACY_KEY)\n",
    "\n",
    "    else:\n",
    "        accuracies = stat_metrics.groupby(NUM_ROUND_KEY, as_index=False).mean()\n",
    "        stds = stat_metrics.groupby(NUM_ROUND_KEY, as_index=False).std()\n",
    "    \n",
    "    percentile_10 = stat_metrics.groupby(NUM_ROUND_KEY, as_index=False).apply(lambda x: x.quantile(0.10)) #.quantile(10)\n",
    "    percentile_90 = stat_metrics.groupby(NUM_ROUND_KEY, as_index=False).apply(lambda x: x.quantile(0.90)) #.quantile(90)\n",
    "    \n",
    "#     print(accuracies)\n",
    "    return accuracies, percentile_10, percentile_90\n",
    "\n",
    "def get_loss_vs_round_number(stat_metrics, weighted=False):\n",
    "    if weighted:\n",
    "        accuracies = stat_metrics.groupby(NUM_ROUND_KEY).apply(_weighted_mean, 'loss', NUM_SAMPLES_KEY)\n",
    "        accuracies = accuracies.reset_index(name='loss')\n",
    "\n",
    "    else:\n",
    "        accuracies = stat_metrics.groupby(NUM_ROUND_KEY, as_index=False).mean()\n",
    "        stds = stat_metrics.groupby(NUM_ROUND_KEY, as_index=False).std()\n",
    "    \n",
    "    percentile_10 = stat_metrics.groupby(NUM_ROUND_KEY, as_index=False).apply(lambda x: x.quantile(0.10)) #.quantile(10)\n",
    "    percentile_90 = stat_metrics.groupby(NUM_ROUND_KEY, as_index=False).apply(lambda x: x.quantile(0.90)) #.quantile(90)\n",
    "    \n",
    "#     print(accuracies)\n",
    "    return accuracies, percentile_10, percentile_90\n",
    "\n",
    "\n",
    "def _weighted_mean(df, metric_name, weight_name):\n",
    "    d = df[metric_name]\n",
    "    w = df[weight_name]\n",
    "    \n",
    "    try:\n",
    "        return (w * d).sum() / w.sum()\n",
    "    except ZeroDivisionError:\n",
    "        return np.nan\n",
    "fpath = \"data/\"\n",
    "SHOW_WEIGHTED = True # show weighted accuracy instead of unweighted accuracy\n",
    "PLOT_CLIENTS = True\n",
    "\n",
    "stat_file_testbed = fpath + 'metrics_stat_testbed.csv' # change to None if desired\n",
    "stat_file = fpath + 'metrics_stat.csv' # change to None if desired\n",
    "sys_file = fpath + 'metrics_sys.csv' # change to None if desired\n",
    "\n",
    "fstat_metrics_testbed= visualization_utils.load_data(stat_file_testbed)\n",
    "fstat_metrics= visualization_utils.load_data(stat_file)\n",
    "\n",
    "faccuracies_testbed,_, _ = get_accuracy_vs_round_number(fstat_metrics_testbed, True)\n",
    "faccuracies, _, _ = get_accuracy_vs_round_number(fstat_metrics, True)\n",
    "\n",
    "loss_testbed,_, _ = get_loss_vs_round_number(fstat_metrics_testbed, True)\n",
    "loss, _, _ = get_loss_vs_round_number(fstat_metrics, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "losses = []\n",
    "accuracies = []\n",
    "for i, cycle_log in enumerate(cycles_log):\n",
    "    losses.append(cycle_log[0].item())\n",
    "    accuracies.append(cycle_log[1].item())\n",
    "    \n",
    "# axs[0].plot(range(0, len(losses))[:500], losses[:500], label='PyGrid Node ')\n",
    "# axs[0].plot(loss_testbed[NUM_ROUND_KEY][:500], loss_testbed['loss'][:500], label='Testbed')\n",
    "# axs[0].plot(loss[NUM_ROUND_KEY][:500], loss['loss'][:500], label='LEAF')\n",
    "# axs[0].legend(loc='best')\n",
    "\n",
    "    \n",
    "\n",
    "# plt.plot(range(0, len(accuracies))[:500], accuracies[:500], label='PyGrid Node ')\n",
    "plt.plot(faccuracies_testbed[NUM_ROUND_KEY][:500], faccuracies_testbed[ACCURACY_KEY][:500], label='PyTorch')\n",
    "plt.plot(faccuracies[NUM_ROUND_KEY][:500], faccuracies[ACCURACY_KEY][:500], label='Tensorflow')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Round Number')\n",
    "\n",
    "\n",
    "plt.savefig('combined_pytorch_leaf_testbed_accuracy.png')\n",
    "#     print(f\"Cycle {i + 1}:\\tLoss: {np.mean(losses)}\\tAcc: {np.mean(accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    " \n",
    "# plt.plot(range(0, len(losses))[:500], losses[:500], label='PyGrid Node ')\n",
    "plt.plot(loss_testbed[NUM_ROUND_KEY][:500], loss_testbed['loss'][:500], label='PyTorch')\n",
    "plt.plot(loss[NUM_ROUND_KEY][:500], loss['loss'][:500], label='TensorFlow')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Round Number')\n",
    "\n",
    "\n",
    "plt.savefig('combined_pytorch_leaf_testbed_loss.png')\n",
    "#     print(f\"Cycle {i + 1}:\\tLoss: {np.mean(losses)}\\tAcc: {np.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
